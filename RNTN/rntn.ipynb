{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "HOME = os.environ['HOME']\n",
    "LOGDIR = HOME+'/src/rntn/tensorflow/tf_logs'\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_float('learning_rate', 0.003, 'Initial learning rate.')\n",
    "flags.DEFINE_float('lamda', 1.0, 'Regularization parameter.')\n",
    "flags.DEFINE_float('u_range', 0.0001, 'Range for uniform random weights.')\n",
    "flags.DEFINE_integer('max_steps', 3000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_float('cost_threshold', 0.5, 'Stop training if cost falls below this level.')\n",
    "flags.DEFINE_integer('batch_size', 3, 'Training batch size.')\n",
    "flags.DEFINE_integer('wvs', 10, 'Word vector size.')\n",
    "flags.DEFINE_integer('n_labels', 5, 'Number of sentiment categories.')\n",
    "flags.DEFINE_string('data_dir', '../data/files', 'Training data directory.')\n",
    "flags.DEFINE_integer('max_sentence_length', 150, 'Maximum length sentence we can process.')\n",
    "flags.DEFINE_boolean('log_device_placement', False, 'Log device placement')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "FTYPE = np.float32\n",
    "print(FLAGS['max_sentence_length'].value)\n",
    "\n",
    "ACT_LEN = FLAGS['max_sentence_length'].value * FLAGS['batch_size'].value\n",
    "\n",
    "i_is_leaf = 0\n",
    "i_is_root = 1\n",
    "i_left = 2\n",
    "i_right = 3\n",
    "i_parent = 4\n",
    "i_idx = 5\n",
    "i_phrase_id = 6\n",
    "N_INFOS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_info(node):\n",
    "    info = np.array([1 if node.is_leaf else 0,\n",
    "                     1 if node.is_root else 0,\n",
    "                     0, 0, 0, 0, 0]).astype(np.int32).reshape([N_INFOS, 1])\n",
    "    info[i_phrase_id] = node.phrase_id\n",
    "    if not node.is_leaf:\n",
    "        info[i_left] = node.left.idx\n",
    "        info[i_right] = node.right.idx\n",
    "    if not node.is_root:\n",
    "        info[i_parent] = node.parent.idx\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feed_dict(p_bounds, p_nodes, p_labels, sentences):\n",
    "    a_nodes = np.array([node_info(node) for s in sentences for node in s])\n",
    "    a_labels = np.array(\n",
    "        [node.sentiment for s in sentences for node in s]).astype(FTYPE).reshape(\n",
    "        [n_nodes, FLAGS['n_labels'].value, 1])\n",
    "    i_node = 0\n",
    "    a_bounds = []\n",
    "    for s in sentences:\n",
    "        a_bounds.append([i_node, len(s)])\n",
    "        i_node += len(s)\n",
    "    feed_dict = {\n",
    "      p_bounds: np.array(a_bounds, dtype=np.int32),\n",
    "      p_nodes: a_nodes,\n",
    "      p_labels: a_labels,\n",
    "    }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_info_to_nodes(sents, ddict):\n",
    "    for s in sents:\n",
    "        for node in s:\n",
    "            entry = ddict[node.phrase]\n",
    "            node.phrase_id = entry.phrase_id\n",
    "            node.sentiment = np.array(np.mat(entry.sentiment_1hot), FTYPE).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rntn = tf.VariableScope(False, name='rntn')\n",
    "\n",
    "ONE = tf.ones([1, 1], name='fONE')\n",
    "ZERO = tf.zeros([1, 1], name='fZERO')\n",
    "iONE = tf.constant(1, name='iONE')\n",
    "iZERO = tf.constant(0, name='iZERO')\n",
    "NEG1 = tf.constant(-1, name='iNEG1')\n",
    "true = tf.constant(True, tf.bool, name='TRUE')\n",
    "false = tf.constant(False, tf.bool, name='FALSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_weight_variable(shape, name=None):\n",
    "    return tf.Variable(tf.random_normal(shape, 0, FLAGS['u_range'].value), name=name)\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return tf.Variable(tf.random_uniform(shape, -FLAGS['u_range'].value, FLAGS['u_range'].value), name=name)\n",
    "\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    return tf.Variable(tf.zeros(shape, dtype=FTYPE), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('weights'):\n",
    "    V = weight_variable([2 * FLAGS['wvs'].value, 2 * FLAGS['wvs'].value, FLAGS['wvs'].value], name='V')\n",
    "    W = weight_variable([FLAGS['wvs'].value, 2 * FLAGS['wvs'].value], name='W')\n",
    "    Ws = weight_variable([FLAGS['n_labels'].value, FLAGS['wvs'].value], name='Ws')\n",
    "\n",
    "with tf.name_scope('weights/bias'):\n",
    "    Wsb = bias_variable([FLAGS['n_labels'].value, 1], name='Wsb')\n",
    "    Wb = bias_variable([FLAGS['wvs'].value, 1], name='Wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232\n"
     ]
    }
   ],
   "source": [
    "(sentences, dsdict, trains, valids, tests) = load_dataset(FLAGS['data_dir'].value)\n",
    "vocab_size = len(dsdict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_info_to_nodes(sentences, dsdict)\n",
    "n_nodes = sum(len(s) for s in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = tf.placeholder(tf.int32, shape=[len(sentences), 2], name='bounds')\n",
    "nodes = tf.placeholder(tf.int32, shape=[n_nodes, N_INFOS, 1], name='nodes')\n",
    "labels = tf.placeholder(tf.float32, shape=[n_nodes, FLAGS['n_labels'].value, 1], name='labels')\n",
    "\n",
    "f_dict = fill_feed_dict(bounds, nodes, labels, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = weight_variable([len(dsdict), FLAGS['wvs'].value, 1], name='words')\n",
    "act_init = tf.constant(0.0, FTYPE, [ACT_LEN, FLAGS['wvs'].value, 1], name='act_init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vec(i_node):\n",
    "    with tf.op_scope([i_node, nodes], 'word_vec') as scope:\n",
    "        phrase_id = tf.reshape(tf.slice(nodes, tf.stack([i_node, i_phrase_id, 0]), [1, 1, 1]), [],\n",
    "                               name='phrase_id')\n",
    "        wv = tf.slice(words, tf.stack([phrase_id, 0, 0]), [1, -1, -1], name='words_slice')\n",
    "        return tf.reshape(wv, [FLAGS['wvs'].value, 1], name=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leaf(i_node):\n",
    "    with tf.op_scope([i_node, nodes], 'is_leaf') as scope:\n",
    "        is_leaf_field = tf.stack([i_node, i_is_leaf, 0], name='is_leaf_field')\n",
    "        n_slice = tf.slice(nodes, is_leaf_field, [1, 1, -1], name='node')\n",
    "        result = tf.reshape(n_slice, [], name=scope)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_root(i_node):\n",
    "    with tf.name_scope('node_info'):\n",
    "        return tf.reshape(tf.slice(nodes, tf.stack([i_node, i_is_root, 0]), [1, 1, -1]), [],\n",
    "        name='is_root')\n",
    "\n",
    "def idx(i_node):\n",
    "    with tf.name_scope('node_info'):\n",
    "        return tf.reshape(tf.slice(nodes, tf.stack([i_node, i_idx, 0]), [1, 1, -1]), [],\n",
    "                          name='idx')\n",
    "\n",
    "\n",
    "def parent(i_node):\n",
    "    with tf.name_scope('node_info'):\n",
    "        return tf.reshape(tf.slice(nodes, tf.stack([i_node, i_parent, 0]), [1, 1, -1]), [],\n",
    "                          name='parent')\n",
    "\n",
    "def get_left(i_node, name=None):\n",
    "    with tf.op_scope([i_node, nodes], name, 'get_left') as scope:\n",
    "        return tf.reshape(tf.slice(nodes, tf.stack([i_node, i_left, 0]), [1, 1, -1]), [],\n",
    "                          name=scope)\n",
    "\n",
    "def get_right(i_node, name=None):\n",
    "    with tf.op_scope([i_node, nodes], name, 'get_right') as scope:\n",
    "        return tf.reshape(tf.slice(nodes, tf.stack([i_node, i_right, 0]), [1, 1, 1]), [],\n",
    "                          name=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(i_node, acts, offset, name=None):\n",
    "    with tf.op_scope([i_node, acts, offset], name, 'get_activation') as scope:\n",
    "        return tf.reshape(\n",
    "               tf.slice(acts, tf.stack([i_node+offset, 0, 0]), [1, -1, -1], name='activation'),\n",
    "               [FLAGS['wvs'].value, 1], name=scope)\n",
    "\n",
    "\n",
    "def left_activation(i_node, acts, offset, name=None):\n",
    "    with tf.op_scope([i_node, acts, offset], name, 'left_activation') as scope:\n",
    "        return get_activation(get_left(i_node), acts, offset, name=scope)\n",
    "\n",
    "\n",
    "def right_activation(i_node, acts, offset, name=None):\n",
    "    with tf.op_scope([i_node, acts, offset], name, 'right_activation') as scope:\n",
    "        return get_activation(get_right(i_node), acts, offset, name=scope)\n",
    "\n",
    "\n",
    "def get_bounds(i_s):\n",
    "    rec = tf.reshape(tf.slice(bounds, tf.stack([i_s, 0]), [1, 2], name='bounds_slice'),\n",
    "                     [2], name='get_bounds')\n",
    "    return rec[0], rec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rntn_tensor_forward(a, b, V, name=None):\n",
    "    with tf.op_scope([a, b, V], name, 'TensorForward') as scope:\n",
    "        wvs = FLAGS['wvs'].value\n",
    "        a = tf.convert_to_tensor(a, dtype=tf.float32, name='a')\n",
    "        b = tf.convert_to_tensor(b, dtype=tf.float32, name='b')\n",
    "        V = tf.convert_to_tensor(V, dtype=tf.float32, name='V')\n",
    "        ab = tf.concat((a, b), 0, name='ab')\n",
    "        return tf.matmul(\n",
    "            tf.transpose(\n",
    "                tf.reshape(\n",
    "                    tf.matmul(\n",
    "                        tf.transpose(ab, name='ab.T'),\n",
    "                        tf.reshape(V, [wvs * 2, wvs * wvs * 2], name='inter/V_flattened'),\n",
    "                            name='inter/abTxV'),\n",
    "                    [wvs * 2, wvs], name='inter/prod/reshape'),\n",
    "                    name='inter/prod/transpose'),\n",
    "            ab, name=scope)\n",
    "\n",
    "def std_forward(a, weights, bias_weights, name=None):\n",
    "    with tf.op_scope([a, W, Wb], name, 'std_forward') as scope:\n",
    "        a = tf.convert_to_tensor(a, dtype=tf.float32, name='input')\n",
    "        weights = tf.convert_to_tensor(weights, dtype=tf.float32, name='weights')\n",
    "        bias_weights = tf.convert_to_tensor(bias_weights, dtype=tf.float32, name='bias_weights')\n",
    "        biased = tf.concat((weights, bias_weights), 1, name='biased')\n",
    "        return tf.matmul(biased, a, name=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_hidden(a, b):\n",
    "    wvs = FLAGS['wvs'].value\n",
    "    ab = tf.concat((a, b), 0, name='ab')\n",
    "    ab1 = tf.concat((ab, ONE), 0, name='ab1')\n",
    "    # below works for tensor 2d x d x2d\n",
    "    # tfinter = tf.reshape(tf.matmul(tf.transpose(ab),\n",
    "    #                                tf.reshape(V, [wvs*2, wvs*wvs*2])),\n",
    "    #                      [wvs, wvs*2])\n",
    "    # below works for tensor 2d x 2d x d\n",
    "    # inter = tf.transpose(\n",
    "    #     tf.reshape(tf.matmul(tf.transpose(ab, name='ab.T'),\n",
    "    #                          tf.reshape(V, [wvs * 2, wvs * wvs * 2]), name='V_reshaped'),\n",
    "    #                [wvs * 2, wvs]), name='inter')\n",
    "    # h = tf.matmul(inter, ab, name='h')\n",
    "    h = rntn_tensor_forward(a, b, V, name='tensor_forward')\n",
    "    #W_biased = tf.concat(1, (W, Wb), name='W_biased')\n",
    "    #std_forward = tf.matmul(W_biased, ab1, name='std_forward')\n",
    "    #return tf.add(h, std_forward, name='fwd_hidden')\n",
    "    return tf.add(h, std_forward(ab1, W, Wb, name='std_forward'), name='fwd_hidden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_info(i):\n",
    "    with tf.op_scope([i, nodes], 'node_info') as scope:\n",
    "        return tf.reshape(tf.slice(nodes, tf.stack([i, 0, 0]), [1, N_INFOS, -1],\n",
    "                                   name='node_info_slice'), [N_INFOS], name=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_node(i_node, acts, offset):\n",
    "    def f_leaf():\n",
    "        return word_vec(i_node)\n",
    "\n",
    "    def f_nonleaf():\n",
    "        a_left = left_activation(i_node, acts, offset)\n",
    "        a_right = right_activation(i_node, acts, offset)\n",
    "        return fwd_hidden(a_left, a_right)\n",
    "\n",
    "    bool_is_leaf = tf.equal(is_leaf(i_node), 1, name='bool_is_leaf')\n",
    "    return f_act(tf.cond(bool_is_leaf, f_leaf, f_nonleaf, name='cond_leaf_nonleaf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_act(x):\n",
    "    return tf.tanh(x, name='f_act_tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_nodes(i_start, size, acts, offset):\n",
    "    # Note: In the corpus that we've seen, parse trees are always ordered such that\n",
    "    # iteration forward through the list will be in bottom-up order.\n",
    "    # Conversely, iteration in reverse is always top-down.\n",
    "    # This enables a simple iterative algorithm. If this were not the case,\n",
    "    # putting the nodes in order by a postorder traversal would fix it.\n",
    "    def fwd_continue(*parms):\n",
    "        (_, sz, cur, _) = parms\n",
    "        return tf.less(cur, sz, name='cur_le_size')\n",
    "\n",
    "    def forward_prop(*parms):\n",
    "        (i0, sz, cur, act) = parms\n",
    "        with tf.device('/gpu:0'):\n",
    "            gact = act\n",
    "            gcur = cur\n",
    "            next_idx = i0 + gcur\n",
    "        node_out = tf.reshape(forward_node(next_idx, act, offset), [1, FLAGS['wvs'].value, 1], name='node_out')\n",
    "        tf.scatter_add(gact, tf.stack([gcur]), node_out, name='act_update')\n",
    "        act = gact\n",
    "        return [i0, sz, cur + iONE, act]\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        i_start = tf.convert_to_tensor(i_start, dtype=tf.int32, name='i_start')\n",
    "        size = tf.convert_to_tensor(size, dtype=tf.int32, name='size')\n",
    "        iZ = tf.convert_to_tensor(0, dtype=tf.int32, name='ZERO')\n",
    "\n",
    "    while_parms = [i_start, size, iZ, acts]\n",
    "    wresult = tf.while_loop(fwd_continue, forward_prop, while_parms, parallel_iterations=1,\n",
    "                            name='forward_prop_while')\n",
    "    (_, _, _, result) = wresult\n",
    "    return tf.slice(result, [0, 0, 0], tf.stack([size, -1, -1]), name='fwd_prop_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sentence(i_s, acts, offset):\n",
    "    i_start, i_size = get_bounds(i_s)\n",
    "    return forward_prop_nodes(i_start, i_size, acts, offset)\n",
    "\n",
    "\n",
    "def activation_to_sm(a):\n",
    "    return tf.transpose(tf.nn.softmax(\n",
    "      tf.transpose(\n",
    "          activation_to_logits(a), name='act_to_softmax')))\n",
    "\n",
    "def activations_to_sm(acts):\n",
    "    return tf.map_fn(activation_to_sm, acts, name='acts_to_softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_rated(i_s):\n",
    "    i_start, i_size = get_bounds(i_s)\n",
    "    return tf.squeeze(tf.slice(labels,\n",
    "                               tf.stack([i_start, 0, 0]),\n",
    "                               tf.stack([i_size, -1, -1]), name='labels_slice'), [2],\n",
    "                      name='sentence_labels')\n",
    "\n",
    "\n",
    "def rated(i_s):\n",
    "    return tf.argmax(sm_rated(i_s), 1, name='sentence_rating')\n",
    "\n",
    "\n",
    "def predict_sentence(i_s, acts, offset=0):\n",
    "    i_start, i_size = get_bounds(i_s)\n",
    "    fwd_acts = forward_prop_nodes(i_start, i_size, acts, offset)\n",
    "    return tf.argmax(activations_to_sm(fwd_acts), 1, name='predict_sentence')\n",
    "\n",
    "\n",
    "def activation_to_logits(a):\n",
    "    Ws_biased = tf.concat((Ws, Wsb), 1, name='Ws_biased')\n",
    "    a1 = tf.concat((a, ONE), 0, name='a1')\n",
    "    return tf.matmul(Ws_biased, a1, name='act_to_logits')\n",
    "\n",
    "\n",
    "def logits(acts):\n",
    "    logits = tf.map_fn(activation_to_logits, acts, name='logits')\n",
    "    return tf.squeeze(logits, [2])\n",
    "\n",
    "\n",
    "def sentence_logits(start, size, acts, offset):\n",
    "    fwd_acts = forward_prop_nodes(start, size, acts, offset)\n",
    "    return logits(fwd_acts)\n",
    "\n",
    "\n",
    "def cost1(i_s, acts, out_ptr):\n",
    "    start, size = get_bounds(i_s)\n",
    "    s_labels = sm_rated(i_s)\n",
    "    s_logits = sentence_logits(start, size, acts, out_ptr)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(s_logits, s_labels, name='c1xentropy')\n",
    "    return tf.reduce_sum(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost1_batch(i_s, acts, out_ptr):\n",
    "    start, size = get_bounds(i_s)\n",
    "    s_labels = sm_rated(i_s)\n",
    "    predicts = sentence_logits(start, size, acts, out_ptr)\n",
    "    result = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        predicts, s_labels, name='c1bxentropy')\n",
    "    return result, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logs, labs):\n",
    "    ce = tf.nn.softmax_cross_entropy_with_logits(logs, labs)\n",
    "    ce_mean = tf.reduce_mean(ce)\n",
    "    with tf.op_scope([V, W, Ws, Wb, Wsb, words], 'regularization') as scope:\n",
    "        regularizers = tf.square(tf.nn.l2_loss(W) + tf.nn.l2_loss(Wb) +\n",
    "                       tf.nn.l2_loss(Ws) + tf.nn.l2_loss(Wsb) +\n",
    "                       tf.nn.l2_loss(V) + tf.nn.l2_loss(words))\n",
    "    loss = ce_mean + regularizers * FLAGS.lamda\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_labels(indices):\n",
    "    inits = tf.zeros([1, FLAGS.n_labels, 1])\n",
    "\n",
    "    def bl_cond(*parms):\n",
    "        i, idxs, _ = parms\n",
    "        return tf.less(i, tf.size(idxs), name='bl_cond')\n",
    "\n",
    "    def bl_body(*parms):\n",
    "        i, idxs, labs = parms\n",
    "        # really?\n",
    "        i_s = tf.reshape(tf.slice(idxs, tf.stack([i]), [1]), [])\n",
    "        start, size = get_bounds(i_s)\n",
    "        i_labels = tf.slice(labels,\n",
    "                            tf.stack([start, 0, 0]),\n",
    "                            tf.stack([size, -1, -1]), name='labels_slice')\n",
    "        new_labels = tf.cond(tf.equal(i, iZERO),\n",
    "                             lambda: i_labels,\n",
    "                             lambda: tf.concat([labs, i_labels], 0))\n",
    "        return i + iONE, idxs, new_labels\n",
    "    with tf.device('/cpu:0'):\n",
    "        iZ = tf.convert_to_tensor(0, dtype=tf.int32)\n",
    "    while_parms = [iZ, indices, inits]\n",
    "    _, _, results = tf.while_loop(bl_cond, bl_body, while_parms, name='batch_labels')\n",
    "    return tf.squeeze(results, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_logits(indices, acts):\n",
    "    init_outs = tf.zeros([1, FLAGS['wvs'].value, 1])\n",
    "\n",
    "    def logits_continue(*parms):\n",
    "        cur, idxs, _, _, _ = parms\n",
    "        return tf.less(cur, tf.size(idxs), name='logits_continue')\n",
    "\n",
    "    def logits_batch_body(*parms):\n",
    "        i, idxs, ptr, css, act = parms\n",
    "        i_s = tf.reshape(tf.slice(idxs, tf.stack([i]), [1]), [])\n",
    "        start, size = get_bounds(i_s)\n",
    "        outs = forward_prop_nodes(start, size, acts, ptr)\n",
    "        new_css = tf.cond(tf.equal(i, iZERO),\n",
    "                          lambda: outs,\n",
    "                          lambda: tf.concat([css, outs], 0))\n",
    "        return i + iONE, indices, ptr + size, new_css, acts\n",
    "    with tf.device('/cpu:0'):\n",
    "        iZ =  tf.convert_to_tensor(0, dtype=tf.int32)\n",
    "    zero_activations(acts)\n",
    "    while_parms = [iZ, indices, iZ, init_outs, acts]\n",
    "    _, _, _, outs, _ = tf.while_loop(logits_continue, logits_batch_body, while_parms,\n",
    "                                     parallel_iterations=1, name='batch_logits_while')\n",
    "    lumpy_logits = tf.map_fn(activation_to_logits, outs, name='raw_logits')\n",
    "    logits = tf.squeeze(lumpy_logits, [2], name='logits')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logs, labs):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "    Args:\n",
    "      logs: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "      labs: Labels tensor, int32 - [batch_size], with values in the\n",
    "        range [0, NUM_CLASSES).\n",
    "    Returns:\n",
    "      A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "      that were predicted correctly.\n",
    "    \"\"\"\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1)\n",
    "    # of all logits for that example.\n",
    "    labs = tf.argmax(labs, 1)\n",
    "    correct = tf.nn.in_top_k(logs, labs, 1)\n",
    "    # Return the number of true entries.\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(i_s, acts):\n",
    "    return tf.equal(rated(i_s), predict_sentence(i_s, acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "c_proto = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=FLAGS['log_device_placement'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_activations(acts):\n",
    "    acts = tf.zeros_like(acts)\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=c_proto)\n",
    "i_ss = tf.placeholder(tf.int32, name='i_ss')\n",
    "\n",
    "setup_done = False\n",
    "\n",
    "\n",
    "def debug_init():\n",
    "    tf.histogram_summary('activations', activations)\n",
    "    f_dict[i_ss] = random.sample(range(len(trains)), FLAGS.batch_size)\n",
    "    logits = batch_logits(i_ss, activations.ref())\n",
    "    labs = batch_labels(i_ss)\n",
    "    loss = calc_loss(logits, labs)\n",
    "    tf.scalar_summary('cost_summary', loss)\n",
    "    writer = tf.train.SummaryWriter(LOGDIR, sess.graph)\n",
    "    merged = tf.merge_all_summaries()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    return logits, labs, loss, merged, writer\n",
    "\n",
    "ro = tf.RunOptions(trace_level='FULL_TRACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    activations = tf.Variable(tf.zeros([ACT_LEN, FLAGS['wvs'].value, 1], FTYPE), name='activations',\n",
    "                          trainable=False)\n",
    "\n",
    "def run_training(cost_threshold=FLAGS['cost_threshold'].value, max_steps=FLAGS['max_steps'].value):\n",
    "    global setup_done\n",
    "    cost_value = 1e9\n",
    "    accuracy_value = 0.0\n",
    "    # if setup_done is False:\n",
    "    setup_done = True\n",
    "    opt = tf.train.AdamOptimizer()\n",
    "    #opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "    i_trains = [s.idx for s in trains]\n",
    "    i_valids = [s.idx for s in valids]\n",
    "    i_tests = [s.idx for s in tests]\n",
    "    i_all = [s.idx for s in sentences]\n",
    "    logits = batch_logits(i_ss, activations)\n",
    "    labs = batch_labels(i_ss)\n",
    "    loss = calc_loss(logits, labs)\n",
    "    i_ss_accuracy = accuracy(logits, labs)\n",
    "    train_op = opt.minimize(loss)\n",
    "    #tf.histogram_summary('activations', activations)\n",
    "    tf.histogram_summary('samples', i_ss)\n",
    "    tf.scalar_summary('loss', loss)\n",
    "    #tf.scalar_summary('training accuracy', train_accuracy)\n",
    "    tf.scalar_summary('validation accuracy', i_ss_accuracy)\n",
    "    # tf.scalar_summary('test accuracy', test_accuracy)\n",
    "    merged = tf.merge_all_summaries()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter(LOGDIR, sess.graph)\n",
    "    f_dict[i_ss] = random.sample(i_trains, FLAGS.batch_size)\n",
    "    _, cost_value = sess.run([train_op, loss], feed_dict=f_dict)\n",
    "    #sys.exit(0) # DEBUG\n",
    "    #f_dict[valid_ss] = i_valids\n",
    "    print('starting')\n",
    "    accuracy_value = sess.run([i_ss_accuracy], feed_dict=f_dict)\n",
    "    for step in range(max_steps):\n",
    "        f_dict[i_ss] = random.sample(i_trains, FLAGS.batch_size)\n",
    "        _, _, cost_value = sess.run([tf.stack([i_ss]), train_op, loss], feed_dict=f_dict)\n",
    "        f_dict[i_ss] = i_valids\n",
    "        _, valid_accuracy_value = sess.run([loss, i_ss_accuracy], feed_dict=f_dict)\n",
    "        (summ,) = sess.run([merged], feed_dict=f_dict)\n",
    "        writer.add_summary(summ, step)\n",
    "        writer.flush()\n",
    "#         if PY3:\n",
    "#             print('.', end='', flush=True)\n",
    "#         else:\n",
    "#             print('.', end='')\n",
    "#             sys.stdout.flush()\n",
    "        if cost_value < cost_threshold:\n",
    "            return step, cost_value, valid_accuracy_value\n",
    "    return max_steps, cost_value, valid_accuracy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 23:03:05.568734 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.581009 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.597046 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.597857 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.606601 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.614681 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.615324 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.620157 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.630775 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "W0712 23:03:05.639010 139913164425024 ops.py:6515] tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '_lazy_read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-71c900b0727f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-91b941108954>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(cost_threshold, max_steps)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mi_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mi_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_ss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_ss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-063d9670c578>\u001b[0m in \u001b[0;36mbatch_logits\u001b[0;34m(indices, acts)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mwhile_parms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     _, _, _, outs, _ = tf.while_loop(logits_continue, logits_batch_body, while_parms,\n\u001b[0;32m---> 22\u001b[0;31m                                      parallel_iterations=1, name='batch_logits_while')\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mlumpy_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_to_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raw_logits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlumpy_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep-learning/sentiment-analysis/rntn/.env/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2751\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 2753\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   2754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep-learning/sentiment-analysis/rntn/.env/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   2243\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2245\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2246\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep-learning/sentiment-analysis/rntn/.env/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2168\u001b[0m         expand_composites=True)\n\u001b[1;32m   2169\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2170\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2171\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-063d9670c578>\u001b[0m in \u001b[0;36mlogits_batch_body\u001b[0;34m(*parms)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mi_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         new_css = tf.cond(tf.equal(i, iZERO),\n\u001b[1;32m     14\u001b[0m                           \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-68d3a1df713d>\u001b[0m in \u001b[0;36mforward_prop_nodes\u001b[0;34m(i_start, size, acts, offset)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mwhile_parms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     wresult = tf.while_loop(fwd_continue, forward_prop, while_parms, parallel_iterations=1,\n\u001b[0;32m---> 29\u001b[0;31m                             name='forward_prop_while')\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fwd_prop_nodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep-learning/sentiment-analysis/rntn/.env/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2751\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 2753\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   2754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep-learning/sentiment-analysis/rntn/.env/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   2243\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2245\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2246\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep-learning/sentiment-analysis/rntn/.env/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2168\u001b[0m         expand_composites=True)\n\u001b[1;32m   2169\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2170\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2171\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-68d3a1df713d>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(*parms)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnext_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnode_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wvs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'node_out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgcur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'act_update'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miONE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep-learning/sentiment-analysis/rntn/.env/lib/python3.6/site-packages/tensorflow_core/python/ops/state_ops.py\u001b[0m in \u001b[0;36mscatter_add\u001b[0;34m(ref, indices, updates, use_locking, name)\u001b[0m\n\u001b[1;32m    416\u001b[0m     return gen_state_ops.scatter_add(ref, indices, updates,\n\u001b[1;32m    417\u001b[0m                                      use_locking=use_locking, name=name)\n\u001b[0;32m--> 418\u001b[0;31m   return ref._lazy_read(gen_resource_variable_ops.resource_scatter_add(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    419\u001b[0m       \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       name=name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_lazy_read'"
     ]
    }
   ],
   "source": [
    "def seval(expr):\n",
    "    return sess.run(expr, feed_dict=f_dict)\n",
    "\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
